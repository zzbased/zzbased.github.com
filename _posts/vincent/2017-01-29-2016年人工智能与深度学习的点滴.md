## 2016年人工智能与深度学习的点滴



### 一. 介绍深度学习的各种基本方法

[zhihu神经网络技术发展脉络](https://www.zhihu.com/question/34681168) 这篇文章非常赞：感知器->多层感知器（BP算法）->2006年hinton预训练NN->2012CNN ImageNet->RNN->目前的各种变体(Attention, GAN...)

#### 神经网络
- [《神经网络与深度学习》连载](http://mp.weixin.qq.com/s/t-UPSoVnwSD7XT6IxbE-Wg)
- [反向传播算法最全解读，机器学习进阶必看](https://mp.weixin.qq.com/s/tuKHEQTY1WoQJvCtdFR4Tw)
#### CNN
[zhihu-CNN(卷积神经网络)是什么？](https://www.zhihu.com/question/52668301) 文章讲得非常好，后续再补充下自己的观念。


- [MIT在读博士周博磊《理解和利用CNN的内部表征》](https://pan.baidu.com/s/1jI8w5dc)

#### RNN & LSTM & Attention

##### LSTM
	
参考自 [目前最受欢迎的 LSTM 教程：谷歌大脑科学家亲解](http://mp.weixin.qq.com/s/AmOUHGhMvGlZGcyNu_2XjA)，[原文](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/RNN-unrolled.png)
	
图：一个展开的 RNN
	
距离增加时，RNN 能将相关信息串联起来的能力也就越弱。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/LSTM3-SimpleRNN.png)

图：标准RNN结构

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/LSTM3-chain.png)

图：标准LSTM结构。使用了三个门：input gate，forget gate，output gate，RNN的横向操作可以视为累积已发生的事情，LSTM的memory cell机制会选择记忆或者忘记所累积的信息来预测某个时刻的输出。故LSTM可以避免RNN的梯度消失（gradient vanishing）

传统的RNN总是用“覆写”的方式计算状态：![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/equation.png)，其中f(.)表示仿射变换外面再套一个sigmoid。根据求导的链式法则，这种形式直接导致梯度被表示为连乘积的形式，以至于梯度消失。粗略的说，很多个小于1的项连乘就很快的逼近零。

现代的RNN（包括但不限于使用LSTM单元的RNN）使用“累加”的形式计算状态：![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/equation_2.png)，这种累加形式导致导数也是累加形式，因此避免了梯度消失。以上解释来源于[zhihu](https://www.zhihu.com/question/34878706)

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/LSTM3-var-GRU.png)

图：GRU（Gated　Recurrent）结构。基于标准LSTM，将遗忘门与输入门结合在一起，名为「更新门」（update gate），并将 cell 状态与隐藏层状态合并在一起。
	
其他LSTM参考资料：[theano官网LSTM教程](http://deeplearning.net/tutorial/lstm.html)，[zhihu上关于学习LSTM的回答](https://www.zhihu.com/question/29411132)

另外一张参考图，我觉得挺好的。

![](https://raw.githubusercontent.com/zzbased/zzbased.github.com/master/_posts//images/lstm_gru.png)

LSTM的常见应用：
	
	seq2seq和encoder-decoder框架。机器翻译，对话生成，句法分析，图文转换等。

- [首次超越LSTM : Facebook 门卷积网络新模型能否取代递归模型？](http://mp.weixin.qq.com/s/5y-qoCtrDU0S4zIawOP8Mw)

#### GAN(对抗生成网络)
- [深度学习新星：GANs的基本原理、应用和走向](http://mp.weixin.qq.com/s/ctCHvrUZWSvcItsIt1Pi1g)
- [生成对抗网络（GANs ）为什么这么火？盘点它自 2014 年以来的主要技术进展](http://mp.weixin.qq.com/s/G2HXSbAUnbw3OzVHX2PGxQ)
- [苹果首份AI论文横空出世，提出SimGAN训练方法](http://mp.weixin.qq.com/s/EWt8zC1Op44NbeZv7Vf9XA)
- [“GAN之父”Goodfellow解答：关于GAN的11个问题](https://mp.weixin.qq.com/s/P4gAbW9EchsevNX4U9cyqQ)
- [GAN学习指南：从原理入门到制作生成Demo](http://mp.weixin.qq.com/s/dokn34ACSeb2g0HN6yOkYg)
- [学界 | 谷歌新论文提出适应性生成对抗网络AdaGAN：增强生成模型](http://mp.weixin.qq.com/s/DscoK7xv3zL80ykHMq9QzQ)

#### 其他
- [香港科技大学杨强 KDD China 技术峰会演讲：迁移学习的本质与实际应用](http://mp.weixin.qq.com/s/pFu0szAvfQcLdF60LVTpXQ)

### 二. 深度学习的基本应用

#### 自然语言处理

- [深度 | 自然语言处理领域深度学习研究总结：从基本概念到前沿成果](https://mp.weixin.qq.com/s/r8reUBXkWIb1vYMOHQ7h-A)
- [总结 | 2016年最值得读的自然语言处理领域Paper](http://mp.weixin.qq.com/s/xasF4kMRGq2ErnXzefB2wg)
- [华为诺亚方舟实验室主任李航：自然语言处理的未来趋势](http://mp.weixin.qq.com/s/_xI0-6RDujiiwG9lVe8ebg)

#### 机器翻译
- [谷歌的神经翻译系统并不意味着机器翻译到头了](http://mp.weixin.qq.com/s/m5sJ0l4HlokmWZcCDAu9QA)

#### 计算机视觉
- [一周论文 | Image Caption任务综述](http://mp.weixin.qq.com/s/HJAGxoIfh6kbzZSP0uo6Wg)
- [深度学习在计算机视觉领域的前沿进展](http://mp.weixin.qq.com/s/rvi7n6Q20RIDQN9mUTLlDA)
- [用深度神经网络生成以假乱真的“照片”](http://mp.weixin.qq.com/s/jLPkBeTYwOoEV5Yo5sYXoA)

#### 语音
- [从声学模型算法角度总结 2016 年语音识别的重大进步](http://mp.weixin.qq.com/s/MWifz9vXbQwj6LakIk5sRQ)
- [机器之心年度盘点 | 从技术角度，回顾2016年语音识别的发展](http://mp.weixin.qq.com/s/jLPkBeTYwOoEV5Yo5sYXoA)

### 三. 如何在实战中使用深度学习

####开源框架
- [开发者必备：基于 Linux 生态的十大 AI 开源框架](http://mp.weixin.qq.com/s/hFjoh4SajArS2A5m3yJCBg)
- [深度盘点国内四大机器学习开源平台](http://mp.weixin.qq.com/s/H6E2mhkuDsYURQsy5c9flw)
- [深度 | 对比深度学习十大框架：TensorFlow最流行但并不是最好](http://mp.weixin.qq.com/s/kPThwFyYHrHxiJaFe1djbw)
- [学界｜盘点四大民间机器学习开源框架：Theano、Caffe、Torch 和 SciKit-learn](http://mp.weixin.qq.com/s/pv1lfYq8GdPHj0fTvPV6Jg)
- [谷歌、微软、OpenAI等巨头七大机器学习开源项目 看这篇就够了](http://mp.weixin.qq.com/s/0eZ34UwqyGskFVvRQTEcVg)
- [开源|LightGBM：三天内收获GitHub 1000+ 星](http://mp.weixin.qq.com/s/M25d_43gHkk3FyG_Jhlvog)

### 四. 一些深度学习的小技巧

- [如何训练深度神经网络？老司机的 15 点建议](https://mp.weixin.qq.com/s/g3QyAZGxa_b9Gt5fVRpDxw)
- [周志华KDD China技术峰会现场演讲：深度学习并不是在“模拟人脑”](http://mp.weixin.qq.com/s/5YZi2NONhLT5F5Hhbk5psg)

### 五. 深度学习的业界应用

#### 国外

- [谷歌大脑的 2016 实现了哪八个小目标？Jeff Dean 亲自撰文回顾](http://mp.weixin.qq.com/s/kFxBHUSf6gsW9UDTR1izfw)

	基于神经网络的机器翻译，2014年提出《利用神经网络实现序列到序列的学习》；[深度学习算法的应用及有效性：因糖尿病引致的视网膜病变](http://jamanetwork.com/journals/jama/article-abstract/2588763)；自动创作，创作，从音乐到艺术；TensorFlow已成为github上最受欢迎的机器学习项目。
	

- [机器之心独家盘点：2016人工智能领域十大焦点回顾](http://mp.weixin.qq.com/s/gleV9tOthxA6pmup8LapJw)

	- No.1 人机世纪大战：李世石 vs. AlphaGo
	- No.2 终结马路杀手，无人驾驶汽车纷纷上路。[英伟达自动驾驶技术解读](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650718429&idx=1&sn=46214968459af95e85efe12b8a26b11b&scene=21)
	- No.3 人工智能真的危险吗？持续一整年的人工智能威胁论
	- No.4 全球政府开始关注人工智能：一边鼓励一边寻求管制方法
	- No.5 继续超越人类，微软语音识别技术达到专业转录员水平
	- No.6 机器翻译整合神经网络，接二连三实现颠覆性突破。[GNMT](https://arxiv.org/pdf/1609.08144v2.pdf)
	- No.7 人工智能硬件大战打响，巨头 vs. 创业公司。英伟达，2016 年人工智能计算硬件领域的最大赢家；Tensor Processing Unit By google；
	- No.8 斯坦福大学发布人工智能研究百年报告。[pdf](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650718808&idx=1&sn=998d5748605dd82b624f69b8983cd287&scene=21)
	- No.9 人工智能/机器人开始渗透进人们的日常生活
	- No.10 打破产业与学界之间的壁垒，大学与企业共同推进技术进步
	- No.11 更为逼真的语音合成（WaveNet），稳步推进的图像识别，视频预测、阅读理解，生成对抗网络等


- [2016十大 AI 演讲，大牛们都发表了什么真知灼见？](http://mp.weixin.qq.com/s/akM404BHl5YjDN6hFxQv1w)

	Deep Learning：since 1991 by Jürgen Schmidhuber； LeCun, Bengio 和 Hinton 合著的深度学习入门大作《Deep Learning》已经完稿，并于 2016 年年底出版发货。
- [2016 这一年，深度学习开始主宰互联网](http://mp.weixin.qq.com/s/SgJtCXZyee4bULpFNRMtRQ)
- [2016 年 AI 十大失败案例，微软 Tay、特斯拉车祸上榜](http://mp.weixin.qq.com/s/pma1eahpmIO-MGalNrX32w)
- [人工智能 2016 十大里程碑盘点！革命还是泡沫？](http://mp.weixin.qq.com/s/D11liCNJl73gNc0eieZC8w)
- [深度学习在 2016 年都有哪些主要研究进展？（附开源平台地址）| 盘点](http://mp.weixin.qq.com/s/r8z43eVEdxrWhsBgZWNYFQ)

#### 国内
- [余凯年度总结，揭开中国人工智能的真实现状](http://mp.weixin.qq.com/s/59iDcEhsZqmF9VNxcM9T0g)

	2006-2016年这十年间，是深度学习的普及和推广期，但所有成果均为感知方面的东西，如图像识别、语音识别等。而从今年开始，最大的不同像 AlphaGo 和自动驾驶等人工智能系统开始从感知过渡到决策。这些系统基于对这个世界的理解，从而主动优化它的决策机制。因此从感知到决策是最大的变化，人工智能只有做决策才能真正改变世界。

	强化学习的框架相对而言是一个比较黑箱的系统，这与感知不同，在感知方面黑箱一点也可以，但在决策上一定要用白箱的、可理解的方式去做，尤其是自动驾驶领域。

	深度学习、神经网络天然就拥有迁移学习的特性，比如用 ImageNet 去训练网络结构，实际上它的很大一部分参数在其他问题上可以复用。在参数领域的迁移学习，大家都搞的比较清楚，但在结构方面的迁移学习，现在还并不太清楚。

	最不容易成功的是基于纯算法的公司，主要原因是壁垒低。无论是平台还是产品，其核心竞争力在这两方面：巨大的市场需求；足够的差异化和独特性，不可复制性
- [2016中国人工智能大事件：从百度深度学习平台到中国脑计划](http://mp.weixin.qq.com/s/rY3Wxti-m4_guQzY4rvL1w)
- [百度如何用人工智能做金融？| KDD China](http://mp.weixin.qq.com/s/q5ZIcCrYpRub05JNeLhvzQ)
- [47个月，百度的人工智能路](https://mp.weixin.qq.com/s/KYBmrPj3zhwF-zaMSiJOmA)

	2015年12月，深度语音识别系统Deep Speech 2，识别准确率可达到97%；百度无人驾驶；[paddlepaddle](http://www.paddlepaddle.org/)发布。
	

#### 创业公司
- [这些在2016年融了钱的明星AI初创公司，今年都在干嘛？](http://mp.weixin.qq.com/s/8sLOGzP39vowEOTCUpdvTg)
- [第四范式CEO戴文渊：大数据不再是AI发展瓶颈，未来企业赢在“维度”](https://mp.weixin.qq.com/s/MVwVsc8KycHcVHhBKH1pOw)

### 附录
- 学习一下paddlepaddle。





