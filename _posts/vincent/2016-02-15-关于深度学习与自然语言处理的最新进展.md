---
layout: post
category : 深度学习
tagline: "Supporting tagline"
tags : [深度学习, NLP]
---
{% include JB/setup %}

最近在阅读微信公众号"布洛卡区"的文章，看到几篇关于深度学习在自然语言处理上的应用，受益很多，记录在这里。

布洛卡区是由张俊林在维护的，目前是畅捷通公共服务部总监，中科院软件所自然语言处理专业博士。

从他的专访中，[专访张俊林：十年程序员的感悟与算法之路](http://www.csdn.net/article/2015-10-29/2826075)摘录了以下信息：

- 布隆过滤器是判断海量数据集合中某条数据是否存在的利器，尤其是空间利用率相当高，能够用小内存处理大数据。跳表用于高效维护序列数据，使用场景也非常多，比如LevelDB维护内存数据时用的就是SkipList，再比如Redis的Sorted Set以及Lucene维护倒排索引都是用的它；LSM树是很多NoSQL系统的核心构件，比如BigTable、Cassandra、RAMCloud等，非常重要。
- 大数据日知录
- 从最近两年的研究进展来看，CNN（卷积神经网络）、RNN和及RNN的改进模型LSTM很可能会成为将来NLP领域最主流的方法，起到类似过去CRF在NLP领域中的作用。CNN和RNN能够很好地融合更多历史信息来解决当前的问题，这是传统统计方法很不容易做好的地方，再加上word embedding能够直接将语义编码，而且解决了传统NLP中的数据稀疏问题。

从他的微博中，还了解到这一活动，SDCC 2015算法专场，有必要把这个会议的知识了解一下。[知名互联网公司的算法实践](http://www.csdn.net/article/2015-11-26/2826331)。


### [深度学习与自然语言处理之四：卷积神经网络模型（CNN）](http://blog.csdn.net/malefactor/article/details/50519566)

CNN基础模型：

- cnn模型最初是用在图像上的，二维数据输入，利用"权值共享，局部感受野"这两个特性，降低网络模型复杂度。
- 在文本处理里，可以先将词embedding化，这样就变成二维输入了。
- 卷积层由不同窗口大小的filter构成；filter的个数可以自己决定；同一个filter的参数是共享的，极大减少参数个数；一个filter就是一类特征识别器；窗口大小其实就是识别的n-gram信息。
- pooling层一般是对每个filter向量取max值；filter个数决定了pooling层的维度；
- 最后再接一下全连接层，并使用dropout技巧。

单CNN的改进

- 扩充输入层维度，引入新特征。前面只提到了word embedding matrix，还可以有entity type / position embedding matrix等。
- 其他模型输出作为CNN输入。
- convolution层的非线性化改造。多层convolution连接，最后多层convolution一起输入到softmax层。
- k-max pooling，保留更多特征
- 分段pooling。一个feature窗口，窗口的前半部分和后半部分可以单独取max pooling。
- 全连接层，ranking而非softmax。

多CNN组合

- 不同角度分析同一个输入
- 一个输入一个CNN

CNN在表达句子之间的关系要比RNN自然

- CNN：一个句子对应一个CNN。

缺点：cnn无法对位置信息和顺序信息进行建模。可能的解决方法是，把位置信息显式作为输入传进去。


### [利用卷积神经网络（CNN）构造社区问答系统](http://blog.csdn.net/malefactor/article/details/50374237)

这篇文章是CNN在文本处理上的一个具体应用，还是挺实用的，本质上个判断两个句子是否语义等价的问题。

常规思路如下所示：

![](http://img.blog.csdn.net/20151221204445517?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

首先把两个要判断语义是否等价的句子转换为Word Embedding形式，作为整个神经网络的输入层，然后CNN1通过卷积层和池化层来抽取出一个句子的语义特征，CNN2抽取出另外一个句子的语义特征，之后两个CNN的池化层拼接起来作为后续三层神经网络的输入层，后续三层神经网络通过隐层对两组语义特征进行非线性变换，最后通过线性层分类输出，得出两个句子是语义相同（比如输出1）或者语义不同（比如输出0）的分类结果。

换种思路来做这个任务，能不能把输入层改造成真正的二维结构，就像一张图片那样，然后套上一个CNN来解决这个问题呢？具体请看文中的方法。

### [深度学习与自然语言处理之五：从RNN到LSTM](http://blog.csdn.net/malefactor/article/details/50436735)

RNN：

- 适合解决时间序列输入输出问题。

![](http://img.blog.csdn.net/20151230185146316?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

BiRNN比RNN多看到了下文。

RNN的问题：vanishing gradient problem。

LSTM: long short-term memory。与RNN的结构是一样的，只是对隐层单元内部结构进行了复杂化改造。LSTM为何优于RNN，因为其解决了长程依赖问题。

GRNN：gated rnn。简化版本的LSTM，模型参数少，不容易过拟合。

常用于 sequence2sequence机器翻译，对话机器人等。

[MXnet版的LSTM 25行python](https://github.com/dmlc/mxnet/blob/master/example/rnn)

### [使用深度双向LSTM模型构造社区问答系统](http://blog.csdn.net/malefactor/article/details/50669741)

![](http://img.blog.csdn.net/20160215181252104)

深度双向LSTM模型的结构如上图所示。强调一下，这个结构也是一个解决对比两个句子相似性的通用RNN解决方案，不仅仅能够使用在问答社区，凡是涉及到对比两个句子或者实体关系的场合完全可以套用这个模型来解决

### [自然语言处理中的Attention Model：是什么及为什么](http://blog.csdn.net/malefactor/article/details/50550211)

Encoder-Decoder是个非常通用的计算框架，至于Encoder和Decoder具体使用什么模型都是由研究者自己定的，常见的比如CNN/RNN/BiRNN/GRU/LSTM/Deep LSTM等，这里的变化组合非常多

抽象的encoder-decoder框架：

![](http://img.blog.csdn.net/20160120181545780)

引入AM模型的Encoder-Decoder框架

![](http://img.blog.csdn.net/20160120181841922)

一般文献里会把AM模型看作是单词对齐模型。

具体参考：A Neural Attention Model for Sentence Summarization。

### [以Attention Model为例谈谈两种研究创新模式](http://blog.csdn.net/malefactor/article/details/50583474)

模型创新与应用创新

### [为何谷歌围棋AI AlphaGo可能会把李世石击溃](http://blog.csdn.net/malefactor/article/details/50631180)

